{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f8eed-879e-49b4-b4b2-8d09abd2eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import openai\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8a6fb-80cb-45aa-83f0-c88a286bcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "#openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "model_engine = \"your_selected_model\"\n",
    "prompt_name = 'your_prompt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a7b41-1a72-49de-a960-b2a44a714512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input():\n",
    "    file_path = 'Prompt/'+ prompt_name +'.txt'\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        user_input = file.read()\n",
    "\n",
    "    dataset = \"your_dataset\"\n",
    "    temperature = 0.0\n",
    "    query = user_input\n",
    "\n",
    "    # Printing the extracted values\n",
    "    print(dataset)\n",
    "    print(temperature)\n",
    "    print(query)\n",
    "    \n",
    "    return dataset, temperature,  query\n",
    "\n",
    "dataset, temperature, query = user_input()\n",
    "Implementation_base_path_input = 'your_dataset_path'\n",
    "print(Implementation_base_path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0351f-fe9e-46ee-b9a7-74cde3839825",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing  ###\n",
    "# List all .txt files in the directory\n",
    "filenames = [f for f in os.listdir(Implementation_base_path_input) if f.endswith('.txt')]\n",
    "\n",
    "# Process each file\n",
    "for filename in filenames:\n",
    "    full_path = os.path.join(Implementation_base_path_input, filename)\n",
    "    with open(full_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Clean the data\n",
    "    cleaned_value = data.replace(\"|\", \" | \")\n",
    "\n",
    "    # Write the cleaned data back to the file\n",
    "    with open(full_path, 'w') as file:\n",
    "        file.write(cleaned_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239d76a-c8a2-4c70-91d9-eed315fb0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_input_tokens(Implementation_base_path_input)\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from math import ceil\n",
    "\n",
    "def count_input_tokens_no_use(folder_path):\n",
    "  print(folder_path)\n",
    "  # Initialize GPT2 tokenizer\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  # Get list of file names in folder\n",
    "  file_names = os.listdir(folder_path)\n",
    "  # List to store file names with more than 1500 tokens\n",
    "  large_token_files = []\n",
    "  x = 0\n",
    "  y = 0\n",
    "  parts = 0\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".csv\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as fp:\n",
    "              num_input_line = len(fp.readlines())\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          num_output_tokens  = num_input_line*100\n",
    "          total_token = num_tokens + num_output_tokens\n",
    "          if total_token > 16000:\n",
    "            print(f\"{file_name}: {total_token} tokens \\t parts needed: {ceil(num_output_tokens/4000)}\")\n",
    "            x= x+1\n",
    "            parts = parts + ceil(num_output_tokens/4000)\n",
    "            large_token_files.append(file_name)\n",
    "          else:\n",
    "            y = y+1\n",
    "            parts = parts +1\n",
    "        \n",
    "  print(x)\n",
    "  print(y)\n",
    "  print(parts)\n",
    "  return large_token_files\n",
    "              \n",
    "            \n",
    "large_token_files = count_input_tokens_no_use(Implementation_base_path_input)\n",
    "print(large_token_files)\n",
    "#### Split the files with large tokens to meet the context window of your desired model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453f4c8-adca-456c-8148-e73021c790f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a0f67-47f6-47f2-bd6f-2d9d10ef8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "import openai\n",
    "\n",
    "def get_completion(BACKOFF_OCCURRED_, Sentences, max_tokens, temperature, query=query, model=model_engine):\n",
    "    prompt = f\"\"\"\n",
    "    {query}\n",
    "    {Sentences}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    #print(prompt)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    tries = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            call_time = time.time()\n",
    "            time_f = call_time - start_time\n",
    "            break\n",
    "        except (openai.error.RateLimitError, openai.error.ServiceUnavailableError, openai.error.APIError)  as e:\n",
    "            tries += 1\n",
    "            max_backoff = 60  # Example: maximum of 60 seconds\n",
    "            backoff_time =  min(5 + 5*(tries ** 2), max_backoff)\n",
    "            #BACKOFF_OCCURRED_ = True # Reset the backoff \n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = True\n",
    "            #print(f\"Rate limit exceeded. Retrying in {backoff_time} seconds...\")\n",
    "            #print(\"############################\\nNumber of Failure:\", tries)\n",
    "            #print(f\"###########################\\nRate limit exceeded. Retrying in {backoff_time} seconds...\")\n",
    "            \n",
    "            time.sleep(backoff_time)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = False\n",
    "            print(\"Backoff Released\\n\")\n",
    "    \n",
    "    message = response['choices'][0]['message']['content']\n",
    "    output_token = response['usage']['completion_tokens']\n",
    "    input_token = response['usage']['prompt_tokens']\n",
    "    \n",
    "    now_utc = datetime.datetime.now(pytz.utc)\n",
    "    timezone = pytz.timezone(\"US/Central\")\n",
    "    now_eastern = now_utc.astimezone(timezone)\n",
    "    time_stamp = str(now_eastern)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return message, input_token, output_token, time_f, time_stamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d1576-e2f0-4d15-b98e-c5341389ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "\n",
    "def call_ChatGPT(BACKOFF_OCCURRED_, Implementation_base_path_input, Implementation_base_path_output, Run, temperature, query, input_file_name):\n",
    "    #print(\"Maximum number of Token: \", total_max_tokens)\n",
    "    #print(\"Temperature: \", temperature)\n",
    "    #print(\"Query:\", query)\n",
    "    #print(\"Dataset: \", dataset)\n",
    "    #print (\"Number of folds:\", D_folds)\n",
    "    \n",
    "    #r = D_folds+1\n",
    "    \n",
    "    RETRY_COUNT = 5\n",
    "    ########################################################################\n",
    "    #if BACKOFF_OCCURRED_:\n",
    "    while BACKOFF_OCCURRED_.value:\n",
    "        print(\"\\nBackoff occurred! Pausing all threads for a set duration...\")\n",
    "        sleep_time = random.randint(1, 5)\n",
    "        time.sleep(sleep_time) # Adjust this duration as per your requirements.\n",
    "        #BACKOFF_OCCURRED_ = False\n",
    "        #BACKOFF_OCCURRED_.value = False\n",
    "            \n",
    "    #########################################################################    \n",
    "            \n",
    "    for attempt in range(RETRY_COUNT):\n",
    "        try:\n",
    "\n",
    "        #for filename in input_file_names:\n",
    "            with open(os.path.join(Implementation_base_path_input, input_file_name)) as f:\n",
    "                Sentences = f.read()\n",
    "\n",
    "            num_lines = sum(1 for line in open(os.path.join(Implementation_base_path_input, input_file_name)))\n",
    "\n",
    "            total_max_tokens = num_lines*100+50\n",
    "            print(total_max_tokens)\n",
    "\n",
    "            message, input_token, output_token, time_f, time_stamp = get_completion(\n",
    "                BACKOFF_OCCURRED_, Sentences=Sentences, max_tokens=total_max_tokens, temperature=temperature\n",
    "            )\n",
    "\n",
    "            last_line = message.strip().split('\\n')[-1]\n",
    "\n",
    "            if \"Done\" in last_line:\n",
    "                print(f\"File = {input_file_name}, Temperature = {temperature}, Complete\")\n",
    "            else:\n",
    "                print(f\"File = {input_file_name}, Temperature = {temperature}, Possibly Incomplete\")\n",
    "\n",
    "            file_name = os.path.basename(input_file_name)\n",
    "            base_name = os.path.splitext(file_name)[0] + \".txt\"\n",
    "\n",
    "            output_file = os.path.join(Implementation_base_path_output, base_name)\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                print(message, file=f)\n",
    "            break\n",
    "                \n",
    "        except Exception as e:  # Catch general exceptions. Be specific if you know which exceptions to expect\n",
    "            print(f\"Error occurred: {e}. Retrying {attempt+1}/{RETRY_COUNT}. input_file_name: {input_file_name}\")\n",
    "            time.sleep(1)  # Wait for 5 seconds before retrying. Adjust as necessary.\n",
    "\n",
    "        else:  # This block will be executed if the for loop completed without 'break', i.e., if all attempts failed.\n",
    "            print(f\"All {RETRY_COUNT} retries failed for fold {current_folds} at Run {Run}. input_file_name: {input_file_name}\")\n",
    "        #time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a8ecd-a3dc-4141-9921-3baf0b4053be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56350891-8792-4940-a683-04a29aaccfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def output_path(Run_no, dataset, temperature):\n",
    "    base = \"Output/\"+prompt_name+\"/GPT_\"+model_engine+\"/\" + dataset + \"_T\"+ str(temperature)+ '/'\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_Run\" + str(Run_no) + '/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "\n",
    "    # Use exist_ok=True to avoid FileExistsError in a multiprocessing context\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "        \n",
    "    return Implementation_base_path_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d41cb-22a5-4125-bcb1-bac0d4d07391",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_names = []\n",
    "\n",
    "for file_name in os.listdir(Implementation_base_path_input):\n",
    "    \n",
    "    if os.path.isfile(os.path.join(Implementation_base_path_input, file_name)):\n",
    "        input_file_names.append(file_name)\n",
    "\n",
    "file_count = len(input_file_names)\n",
    "\n",
    "print(\"Number of files in the folder:\", file_count)\n",
    "#print(\"File names:\", input_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24afc9-c966-4cee-a44b-a9881569a540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1f513-f4a7-40f8-9d8f-98489686e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e7934-3f23-4636-ad2f-b31ec891a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Setting up the shared variable\n",
    "BACKOFF_OCCURRED_ = multiprocessing.Value('b', False)  # 'b' denotes a boolean\n",
    "\n",
    "def process_file(input_file_name, Run, dataset, temperature, query):\n",
    "    \n",
    "    global BACKOFF_OCCURRED_\n",
    "    \n",
    "    Implementation_base_path_output = output_path(Run, dataset,temperature)\n",
    "    with open(os.path.join(Implementation_base_path_input, input_file_name)) as f:\n",
    "        Sentences = f.read()\n",
    "        \n",
    "    if os.path.exists(os.path.join(Implementation_base_path_input, input_file_name)):\n",
    "        with open(os.path.join(Implementation_base_path_input, input_file_name)) as f:\n",
    "            Sentences = f.read()\n",
    "        call_ChatGPT(BACKOFF_OCCURRED_, Implementation_base_path_input, Implementation_base_path_output, Run, temperature, query, input_file_name)\n",
    "    else:\n",
    "        print(f\"File not found: {os.path.join(Implementation_base_path_input, input_file_name)}\")\n",
    "\n",
    "def worker_wrapper(args):\n",
    "    return process_file(*args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup your variables: Run, dataset, temperature, query, input_file_names\n",
    "    number_of_workers = 10  # Number of worker processes\n",
    "    for Run in range(1, 4):\n",
    "        # Arguments to pass to process_file for each input_file_name\n",
    "        args_list = [(file_name, Run, dataset, temperature, query) for file_name in input_file_names]\n",
    "\n",
    "        # Create a pool of worker processes\n",
    "        with multiprocessing.Pool(processes=number_of_workers) as pool:\n",
    "            # map function blocks until all results are ready\n",
    "            pool.map(worker_wrapper, args_list)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
