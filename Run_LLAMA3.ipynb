{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f8eed-879e-49b4-b4b2-8d09abd2eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8a6fb-80cb-45aa-83f0-c88a286bcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31341c1-466d-49f6-99a7-2f23a81170da",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"your_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2cb0d-a9e8-440f-bdb0-9a9928fc0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"huggingface_model_name\" \n",
    "model_engine = \"model_engine_spec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d5e50-9e91-4e02-a5f3-584a85cdda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecca8f-734f-44d3-9118-374103b3e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          token=HF_TOKEN)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccd153-bda3-4a8b-94fc-0d960ed66c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "print(\"N GPUS: \", n_gpus)\n",
    "max_memory = f'{40960}MB'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN,\n",
    "    max_memory = {i: max_memory for i in range(n_gpus)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afefed-87b2-4937-845f-11a120fedefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=4096,\n",
    "    #do_sample=False,\n",
    "    temperature=0.1,\n",
    "    #top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8c852-05f6-4799-8c7e-4229bc7879cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_LLAMA(prompt):\n",
    "  sequences = text_generator(prompt)\n",
    "  gen_text = sequences[0][\"generated_text\"]\n",
    "  return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a7b41-1a72-49de-a960-b2a44a714512",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## User input in txt file #####################\n",
    "def user_input():\n",
    "    #file_path = 'input.txt'\n",
    "    file_path = 'prompt_path'\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        user_input = file.read()\n",
    "\n",
    "    dataset = \"dataset_name\"\n",
    "    temperature = 0.0\n",
    "    query = user_input\n",
    "\n",
    "    # Printing the extracted values\n",
    "    print(dataset)\n",
    "    print(temperature)\n",
    "    print(query)\n",
    "    \n",
    "    return dataset, temperature,  query\n",
    "\n",
    "dataset, temperature, query = user_input()\n",
    "\n",
    "if dataset == \"dataset_name\":\n",
    "    Implementation_base_path_input = 'dataset_path'\n",
    "\n",
    "print(Implementation_base_path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239d76a-c8a2-4c70-91d9-eed315fb0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_input_tokens(Implementation_base_path_input)\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from math import ceil\n",
    "\n",
    "folder_path = Implementation_base_path_input\n",
    "\n",
    "def count_input_tokens_no_use(folder_path):\n",
    "  print(folder_path)\n",
    "  # Initialize GPT2 tokenizer\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  # Get list of file names in folder\n",
    "  file_names = os.listdir(folder_path)\n",
    "  # List to store file names with more than 1500 tokens\n",
    "  large_token_files = []\n",
    "  x = 0\n",
    "  y = 0\n",
    "  parts = 0\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as fp:\n",
    "              num_input_line = len(fp.readlines())\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          num_output_tokens  = num_input_line*100\n",
    "          total_token = num_tokens + num_output_tokens\n",
    "          if total_token > 16000:\n",
    "            print(f\"{file_name}: {total_token} tokens \\t parts needed: {ceil(num_output_tokens/4000)}\")\n",
    "            x= x+1\n",
    "            parts = parts + ceil(num_output_tokens/4000)\n",
    "            large_token_files.append(file_name)\n",
    "          else:\n",
    "            y = y+1\n",
    "            parts = parts +1\n",
    "        \n",
    "  print(x)\n",
    "  print(y)\n",
    "  print(parts)\n",
    "  return large_token_files\n",
    "              \n",
    "            \n",
    "large_token_files = count_input_tokens_no_use(folder_path)\n",
    "print(large_token_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a8ecd-a3dc-4141-9921-3baf0b4053be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold\n",
    "def call_LLM(Implementation_base_path_input, Implementation_base_path_output, Run, temperature, query):\n",
    "    print(\"Run no: \", Run+1)\n",
    "    print(\"Temperature: \", temperature)\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Dataset: \", dataset)\n",
    "    r = 11\n",
    "    for fold_no in range(1, r):\n",
    "        with open(os.path.join(Implementation_base_path_input, f'fold{fold_no}.txt')) as f:\n",
    "            Sentences = f.read()\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        {query}\n",
    "        {Sentences}\n",
    "        Output:\n",
    "                \"\"\"\n",
    "            \n",
    "        message = get_response_LLAMA(prompt)\n",
    "\n",
    "        last_line = message.strip().split('\\n')[-1]\n",
    "        if \"Done\" in last_line:\n",
    "            print(f\"Fold = {fold_no}, Run = {Run}, Temperature = {temperature}, Complete\")\n",
    "        else:\n",
    "            print(f\"Fold = {fold_no}, Run = {Run}, Temperature = {temperature}, Possibly Incomplete\")\n",
    "\n",
    "        with open(\n",
    "            os.path.join(Implementation_base_path_output + str(Run + 1) + f'_fold_{fold_no}.txt'), \"a\"\n",
    "        ) as f:\n",
    "            print(message, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56350891-8792-4940-a683-04a29aaccfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def output_path(Run_no, dataset, temperature):\n",
    "    base = \"Output/\"+model_engine+\"/\" + dataset + \"_T\"+ str(temperature)+ '/'\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_Run\" + str(Run_no) + '/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "\n",
    "    # Use exist_ok=True to avoid FileExistsError in a multiprocessing context\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "        \n",
    "    return Implementation_base_path_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d41cb-22a5-4125-bcb1-bac0d4d07391",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_names = []\n",
    "\n",
    "for file_name in os.listdir(Implementation_base_path_input):\n",
    "    \n",
    "    if os.path.isfile(os.path.join(Implementation_base_path_input, file_name)):\n",
    "        input_file_names.append(file_name)\n",
    "\n",
    "file_count = len(input_file_names)\n",
    "\n",
    "print(\"Number of files in the folder:\", file_count)\n",
    "#print(\"File names:\", input_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1f513-f4a7-40f8-9d8f-98489686e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Run in range(3):\n",
    "    R = Run+1\n",
    "    Implementation_base_path_output = output_path(R, dataset, temperature)\n",
    "    print(\"Implementation base path input:\", Implementation_base_path_input)\n",
    "    print(\"Implementation base path output:\", Implementation_base_path_output)\n",
    "    call_LLM(Implementation_base_path_input, Implementation_base_path_output, Run, temperature, query)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
