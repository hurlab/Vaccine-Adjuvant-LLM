{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8fd7a-0bc9-4a6b-971b-51a875f9cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f8eed-879e-49b4-b4b2-8d09abd2eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2808fb-1c6b-4bfb-9efb-88411a127b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"without_Substances\"\n",
    "shot = \"0shot\"\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "model_engine = \"google-gemma-2-9b-it_llama_prompt2_\"+ setting +\"_\"+ shot + \"/\"\n",
    "temperature =  0.0001\n",
    "max_token = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31341c1-466d-49f6-99a7-2f23a81170da",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"your_token\"\n",
    "login(token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a8f6a-a7cd-46c4-aeb6-4fb797a280de",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## User input in txt file #####################\n",
    "def user_input():\n",
    "    file_path = \"Prompts/Prompt_VAC/\"+ setting +\"_\" + shot + \"_Single_Abstract.txt\"\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        user_input = file.read()\n",
    "    dataset = \"VAC_Cancer_Vaccine_adjuvant\"\n",
    "    query = user_input\n",
    "\n",
    "    print(dataset)\n",
    "    print(temperature)\n",
    "    print(query)\n",
    "    \n",
    "    return dataset, temperature,  query\n",
    "\n",
    "dataset, temperature, query = user_input()\n",
    "\n",
    "\n",
    "#Single_Abstract_preprocessed_with_Substances_only_with_substances_merged_columns\n",
    "if dataset == \"VAC_Cancer_Vaccine_adjuvant\":\n",
    "    Implementation_base_path_input = 'Dataset/'+dataset+'/Single_Abstract_preprocessed_with_Substances_only_' + setting + '_merged_columns'\n",
    "\n",
    "print(Implementation_base_path_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d08d3c-bd04-48ab-87c2-b2b06c5b6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected code with max_memory in the right place\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(\"N GPUS: \", n_gpus)\n",
    "max_memory = f'{40960}MB'\n",
    "\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"max_memory\": {i: max_memory for i in range(n_gpus)}  # Moved here\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8c852-05f6-4799-8c7e-4229bc7879cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_LLAMA(prompt):\n",
    "  \n",
    "  messages = [\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "  ]\n",
    "  sequences = text_generator(messages, max_new_tokens=max_token)\n",
    "  gen_text = sequences[0][\"generated_text\"][-1][\"content\"]\n",
    "  return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239d76a-c8a2-4c70-91d9-eed315fb0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_input_tokens(Implementation_base_path_input)\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from math import ceil\n",
    "\n",
    "folder_path = Implementation_base_path_input\n",
    "\n",
    "\n",
    "def count_input_tokens_no_use(folder_path):\n",
    "  print(folder_path)\n",
    "  # Initialize GPT2 tokenizer\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  # Get list of file names in folder\n",
    "  file_names = os.listdir(folder_path)\n",
    "  # List to store file names with more than 1500 tokens\n",
    "  large_token_files = []\n",
    "  x = 0\n",
    "  y = 0\n",
    "  parts = 0\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as fp:\n",
    "              num_input_line = len(fp.readlines())\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          num_output_tokens  = num_input_line*100\n",
    "          total_token = num_tokens + num_output_tokens\n",
    "          if total_token > 16000:\n",
    "            print(f\"{file_name}: {total_token} tokens \\t parts needed: {ceil(num_output_tokens/4000)}\")\n",
    "            x= x+1\n",
    "            parts = parts + ceil(num_output_tokens/4000)\n",
    "            large_token_files.append(file_name)\n",
    "          else:\n",
    "            y = y+1\n",
    "            parts = parts +1\n",
    "        \n",
    "  print(x)\n",
    "  print(y)\n",
    "  print(parts)\n",
    "  return large_token_files\n",
    "              \n",
    "            \n",
    "large_token_files = count_input_tokens_no_use(folder_path)\n",
    "print(large_token_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453f4c8-adca-456c-8148-e73021c790f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c1b32-0ab2-483c-97ec-4d4027c0c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_txt_files_in_folder(folder_path):\n",
    "    txt_file_count = 0\n",
    "    # os.walk() generates the file names in a directory tree by walking either top-down or bottom-up.\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                txt_file_count += 1  # Increment the count if the file ends with .txt\n",
    "    return txt_file_count\n",
    "\n",
    "def count_txt_files_excluding_checkpoints(folder_path):\n",
    "    txt_file_count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and '-checkpoint' not in file:\n",
    "                txt_file_count += 1\n",
    "    return txt_file_count\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path_original = Implementation_base_path_input\n",
    "\n",
    "total_txt_files_original = count_txt_files_excluding_checkpoints(folder_path_original)\n",
    "print(f\"Total number of original .txt files: {total_txt_files_original}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a8ecd-a3dc-4141-9921-3baf0b4053be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold\n",
    "def call_LLM(Implementation_base_path_input, Implementation_base_path_output, Run, temperature, query):\n",
    "    print(\"Run no: \", Run+1)\n",
    "    #print(\"Temperature: \", temperature)\n",
    "    #print(\"Query:\", query)\n",
    "    #print(\"Dataset: \", dataset)\n",
    "    r = 291\n",
    "    for fold_no in range(1, r):\n",
    "        with open(os.path.join(Implementation_base_path_input, f'Abstract_{fold_no}.txt')) as f:\n",
    "            Sentences = f.read()\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        {query}\n",
    "        {Sentences}\n",
    "        ### Task Output:\"\"\"\n",
    "\n",
    "        print(prompt)\n",
    "            \n",
    "        message = get_response_LLAMA(prompt)\n",
    "\n",
    "        last_line = message.strip().split('\\n')[-1]\n",
    "        if \"Done\" in last_line:\n",
    "            print(f\"Abstract no= {fold_no}, Run = {Run}, Temperature = {temperature}, Complete\")\n",
    "        else:\n",
    "            print(f\"Abstract no= {fold_no}, Run = {Run}, Temperature = {temperature}, Possibly Incomplete\")\n",
    "\n",
    "        with open(\n",
    "            os.path.join(Implementation_base_path_output + str(Run + 1) + f'_Abstract_{fold_no}.txt'), \"a\"\n",
    "        ) as f:\n",
    "            print(message, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56350891-8792-4940-a683-04a29aaccfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def output_path(Run_no, dataset, temperature):\n",
    "    base = \"Output/\"+model_engine+\"/Single_Abstract_With_Substances_Only_\" + dataset + \"_T\"+ str(temperature)+ '_16December/'   ######################## without\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_Run\" + str(Run_no) + '/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "\n",
    "    # Use exist_ok=True to avoid FileExistsError in a multiprocessing context\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "        \n",
    "    return Implementation_base_path_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d41cb-22a5-4125-bcb1-bac0d4d07391",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_names = []\n",
    "\n",
    "for file_name in os.listdir(Implementation_base_path_input):\n",
    "    \n",
    "    if os.path.isfile(os.path.join(Implementation_base_path_input, file_name)):\n",
    "        input_file_names.append(file_name)\n",
    "\n",
    "file_count = len(input_file_names)\n",
    "\n",
    "print(\"Number of files in the folder:\", file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1f513-f4a7-40f8-9d8f-98489686e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Run in range(3):\n",
    "    R = Run+1\n",
    "    Implementation_base_path_output = output_path(R, dataset, temperature)\n",
    "    print(\"Implementation base path input:\", Implementation_base_path_input)\n",
    "    print(\"Implementation base path output:\", Implementation_base_path_output)\n",
    "    call_LLM(Implementation_base_path_input, Implementation_base_path_output, Run, temperature, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1f4c9-e44a-4e91-831c-154d381ac32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (unsloth_env)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
