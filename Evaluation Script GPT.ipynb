{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d7c36-17cd-48de-bc52-f21629751cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_text(content):\n",
    "    content = content.replace(\"Output:\", \"\").replace(\"```\", \"\")\n",
    "    content = content.replace(\"Done\\tDone\", \"\").replace(\"Done\", \"\")\n",
    "    content = content.replace(\"\\n\\n\", \"\\n\")\n",
    "    content = content.replace(\"\\n\\nNCT Number\\tAdjuvant Name\", \"NCT Number\\tAdjuvant Name\")\n",
    "    content = content.replace(\"\\nNCT Number\\tAdjuvant Name\", \"NCT Number\\tAdjuvant Name\")\n",
    "    header = \"NCT Number\\tAdjuvant Name\\n\"\n",
    "    if not content.startswith(header):\n",
    "        content = header + content\n",
    "    content = content.replace(\"\\n\\n\", \"\\n\")\n",
    "    return content\n",
    "\n",
    "def process_text_file(input_filepath, output_filepath):\n",
    "    with open(input_filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    content = process_text(content)\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "          \n",
    "def process_directory(input_dir, output_dir):\n",
    "    for subdir, dirs, files in os.walk(input_dir):\n",
    "        if \"AdjuvareDB104_T0.0_24June\" in subdir:\n",
    "            for file in files:\n",
    "                if file.endswith('.txt'):\n",
    "                    input_filepath = os.path.join(subdir, file)\n",
    "                    relative_path = os.path.relpath(input_filepath, input_dir)\n",
    "                    output_filepath = os.path.join(output_dir, relative_path)\n",
    "                    process_text_file(input_filepath, output_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90339663-4d82-4dd0-acfb-77f94d4e83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    # Set the input and output directories\n",
    "    input_dir = 'Output/Prompt2_merged_interventions_'+str(i)+'shot'\n",
    "    output_dir = 'Output/Prompt2_merged_interventions_'+str(i)+'shot_postprocessed'\n",
    "    \n",
    "    # Process all .txt files from input directory to output directory\n",
    "    process_directory(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b99950-942c-4fcd-bef3-815abecfa687",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    # Set the input and output directories\n",
    "    input_dir = 'Output/Prompt2_merged_'+str(i)+'shot'\n",
    "    output_dir = 'Output/Prompt2_merged_'+str(i)+'shot_postprocessed'\n",
    "\n",
    "    # Process all .txt files from input directory to output directory\n",
    "    process_directory(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d62a8-db3a-401c-8b4e-db5508aaa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Merge Outputs  ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09945b16-ea1c-4aa4-8e09-24079f13fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(folder_path, output_file):\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        print(folder_path)\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            #print(filename)\n",
    "            df = pd.read_csv(file_path, sep = '\\t')\n",
    "            \n",
    "            # Skip rows containing \"Done\" in any cell\n",
    "            df_filtered = df[~df.apply(lambda row: row.astype(str).str.contains('Done').any(), axis=1)]\n",
    "            dataframes.append(df_filtered)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"All CSV files in {folder_path} have been successfully merged, skipping rows with 'Done'!\")\n",
    "\n",
    "import os\n",
    "\n",
    "models = ['GPT_gpt-3.5-turbo-1106', 'GPT_gpt-3.5-turbo-0125', 'GPT_gpt-4-turbo-2024-04-09']\n",
    "shots = [0, 1, 2, 3, 4]\n",
    "base_folder = 'Output'\n",
    "run_folders = ['AdjuvareDB104_T0.0_Run1', 'AdjuvareDB104_T0.0_Run2', 'AdjuvareDB104_T0.0_Run3']\n",
    "\n",
    "# Iterate over each model and shot combination\n",
    "for model in models:\n",
    "    for shot in shots:\n",
    "        # Construct the base folder path for each model and shot\n",
    "        folder_path = os.path.join(base_folder, f'Prompt2_merged_interventions_{shot}shot_postprocessed', model, 'AdjuvareDB104_T0.0_24June')\n",
    "\n",
    "        # Process each run folder\n",
    "        for run_folder in run_folders:\n",
    "            full_folder_path = os.path.join(folder_path, run_folder)\n",
    "            output_file = os.path.join(folder_path, f'{run_folder}_merged_file.csv')\n",
    "            merge_csv_files(full_folder_path, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e629c2-394a-484d-96a7-eafce059b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "models = ['GPT_gpt-3.5-turbo-1106', 'GPT_gpt-3.5-turbo-0125', 'GPT_gpt-4-turbo-2024-04-09']\n",
    "shots = [0, 1, 2, 3, 4]\n",
    "runs = [(\"AdjuvareDB104_T0.0_Run1\", \"Run1\"), (\"AdjuvareDB104_T0.0_Run2\", \"Run2\"), (\"AdjuvareDB104_T0.0_Run3\", \"Run3\")]\n",
    "goldstandard_path = 'Dataset/AdjuvareDB104_Standard/10_folds_preprocessed_merged_file.csv'\n",
    "\n",
    "def merge_goldstandard_with_predicted(model, shot, run_folder, run_number):\n",
    "    predicted_path = os.path.join('Output', f'Prompt2_merged_interventions_{shot}shot_postprocessed', model, f'AdjuvareDB104_T0.0_18June/{run_folder}_merged_file.csv')\n",
    "    output_path = os.path.join('Output', f'Prompt2_merged_interventions_{shot}shot_postprocessed', model, f'AdjuvareDB104_T0.0_18June/{run_folder}_merged_with_goldstandard.csv')\n",
    "\n",
    "    # Read the gold standard and predicted datasets\n",
    "    goldstandard_df = pd.read_csv(goldstandard_path)\n",
    "    predicted_df = pd.read_csv(predicted_path)\n",
    "\n",
    "    # Perform outer join based on the \"NCT Number\" column\n",
    "    merged_df = pd.merge(goldstandard_df, predicted_df, on=\"NCT Number\", how=\"outer\")\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"The gold standard and predicted datasets for {model}, {shot} shots, {run_number} have been successfully merged with an outer join based on 'NCT Number'!\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Iterate over each model, shot, and run combination\n",
    "all_merged_dfs = {}\n",
    "for model in models:\n",
    "    for shot in shots:\n",
    "        merged_dfs = []\n",
    "        for run_folder, run_number in runs:\n",
    "            merged_df = merge_goldstandard_with_predicted(model, shot, run_folder, run_number)\n",
    "            merged_dfs.append(merged_df)\n",
    "        all_merged_dfs[(model, shot)] = merged_dfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46436cce-d4ca-433f-9a9e-ac2ed3b34602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "models = ['GPT_gpt-3.5-turbo-1106', 'GPT_gpt-3.5-turbo-0125', 'GPT_gpt-4-turbo-2024-04-09']\n",
    "shots = [0, 1, 2, 3, 4]\n",
    "runs = [(\"AdjuvareDB104_T0.0_Run1\", \"Run1\"), (\"AdjuvareDB104_T0.0_Run2\", \"Run2\"), (\"AdjuvareDB104_T0.0_Run3\", \"Run3\")]\n",
    "goldstandard_path = 'Dataset/AdjuvareDB104_Standard/10_folds_preprocessed_merged_file.csv'\n",
    "\n",
    "def merge_goldstandard_with_predicted(model, shot, run_folder, run_number):\n",
    "    predicted_path = os.path.join('Output', f'Prompt2_merged_interventions_{shot}shot_postprocessed', model, f'AdjuvareDB104_T0.0_24June/{run_folder}_merged_file.csv')\n",
    "    output_path = os.path.join('Output', f'Prompt2_merged_interventions_{shot}shot_postprocessed', model, f'AdjuvareDB104_T0.0_24June/{run_folder}_merged_with_goldstandard.csv')\n",
    "\n",
    "    # Read the gold standard and predicted datasets\n",
    "    goldstandard_df = pd.read_csv(goldstandard_path)\n",
    "    predicted_df = pd.read_csv(predicted_path)\n",
    "\n",
    "    # Perform outer join based on the \"NCT Number\" column\n",
    "    merged_df = pd.merge(goldstandard_df, predicted_df, on=\"NCT Number\", how=\"outer\")\n",
    "\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"The gold standard and predicted datasets for {model}, {shot} shots, {run_number} have been successfully merged with an outer join based on 'NCT Number'!\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Iterate over each model, shot, and run combination\n",
    "all_merged_dfs = {}\n",
    "for model in models:\n",
    "    for shot in shots:\n",
    "        merged_dfs = []\n",
    "        for run_folder, run_number in runs:\n",
    "            merged_df = merge_goldstandard_with_predicted(model, shot, run_folder, run_number)\n",
    "            merged_dfs.append(merged_df)\n",
    "        all_merged_dfs[(model, shot)] = merged_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4de3f-c42d-4a21-b001-35237ccb8b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87506fd7-038a-4d6c-ba1d-094d32fff779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Load the CSV file to check its structure and the first few rows\n",
    "file_path = 'Output/Prompt2_merged_interventions_0shot_postprocessed/GPT_gpt-3.5-turbo-1106/AdjuvareDB104_T0.0_18June/AdjuvareDB104_T0.0_Run1_merged_with_goldstandard.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "nan_count = data['Adjuvant Name_x'].isna().sum()\n",
    "print(f'Number of NaN values in Adjuvant Name_x column: {nan_count}')\n",
    "df_cleaned = data.dropna(subset=['Adjuvant Name_x'])\n",
    "#df_cleaned\n",
    "\n",
    "\n",
    "nan_count = df_cleaned['Adjuvant Name_y'].isna().sum()\n",
    "print(f'Number of NaN values in Adjuvant Name_y column: {nan_count}')\n",
    "\n",
    "'''# Remove rows with NaN\n",
    "df_cleaned = df_cleaned.dropna(subset=['Adjuvant Name_y'])'''\n",
    "# Replace NaN values with \"No output Returned\"\n",
    "df_cleaned['Adjuvant Name_y'].fillna(\"No output Returned\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned_label = df_cleaned.copy()\n",
    "\n",
    "# Initialize the 'Match' column with False\n",
    "df_cleaned_label['Match'] = False\n",
    "\n",
    "\n",
    "# Function to clean and split names\n",
    "def clean_and_split(name):\n",
    "    # Lowercase, replace hyphens with spaces, and split by \"and\"\n",
    "    return [part.strip() for part in name.split(\" and \")]\n",
    "\n",
    "# Define a dictionary for abbreviation matching\n",
    "abbreviation_dict = {\n",
    "    \"IFA\": \"Incomplete Freund's Adjuvant\",\n",
    "    \"Incomplete Freund's adjuvant (IFA)\": \"Incomplete Freund's Adjuvant\",\n",
    "    \"rhGM-CSF\": \"GM-CSF\",\n",
    "    \"Sargramostim (GM-CSF)\": \"GM-CSF\",\n",
    "    \"recombinant fowlpox GM-CSF vaccine adjuvant\":\"GM-CSF\", \n",
    "    \"granulocyte-macrophage colony-stimulating factor (GM-CSF)\":\"GM-CSF\", \n",
    "    \"Montanide ISA 51 VG\": \"Montanide ISA 51\",\n",
    "    \"Montanide ISA-51 VG\": \"Montanide ISA 51\",\n",
    "    \"MONTANIDE ISA 51 VG\": \"Montanide ISA 51\",\n",
    "    \"Montanide ISA51 VG\": \"Montanide ISA51\",\n",
    "    \n",
    "    \"Interleukin-2\": \"IL-2\",\n",
    "    \"polyinosinic-polycytidylic acid - poly-L-lysine carboxymethylcellulose (poly-ICLC)\": \"Poly-ICLC\",\n",
    "    \"polyinosinic-polycytidylic acid - poly-L-lysine carboxymethylcellulose (poly-ICLC)\": \"Poly-ICLC\",\n",
    "    \"Hiltonol (Poly-ICLC)\": \"Poly-ICLC\",\n",
    "    \"gp96 heat shock protein-peptide complex\": \"GP96\",\n",
    "    \"Therapeutic Vaccine GI-4000\":\"GI-4000\"\n",
    "    # Add more entries as needed\n",
    " \n",
    "}\n",
    "def resolve_abbreviation(name):\n",
    "    \"\"\"Resolve abbreviations using a predefined dictionary with robust matching.\"\"\"\n",
    "    # Lowercase the name for consistent dictionary lookup\n",
    "    return abbreviation_dict.get(name, name)\n",
    "\n",
    "\n",
    "# Loop through the DataFrame and compare the values using .loc for assignment\n",
    "for index, row in df_cleaned_label.iterrows():\n",
    "    parts_x = clean_and_split(row['Adjuvant Name_x'])\n",
    "    parts_y = clean_and_split(row['Adjuvant Name_y'])\n",
    "    \n",
    "    match_found = False\n",
    "    \n",
    "    for part_x in parts_x:\n",
    "        for part_y in parts_y:\n",
    "            resolved_part_x = resolve_abbreviation(part_x)\n",
    "            resolved_part_y = resolve_abbreviation(part_y)\n",
    "            \n",
    "            if resolved_part_x.lower() == resolved_part_y.lower():\n",
    "                match_found = True\n",
    "                break\n",
    "            elif resolved_part_x.lower().replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\") == resolved_part_y.lower().replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\"):\n",
    "                match_found = True\n",
    "                break\n",
    "            elif resolved_part_x.lower().replace(\"-\", \" \").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\") == resolved_part_y.lower().replace(\"-\", \" \").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\"):\n",
    "                match_found = True\n",
    "                break\n",
    "            elif resolved_part_x.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\") == resolved_part_y.replace(\" \", \"\").lower().replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\"):\n",
    "                match_found = True\n",
    "                break\n",
    "                \n",
    "        if match_found:\n",
    "            break\n",
    "    \n",
    "    df_cleaned_label.loc[index, 'Match'] = match_found\n",
    "\n",
    "#df_cleaned_label\n",
    "\n",
    "# Filter the DataFrame to show only rows where 'Match' is False\n",
    "no_match_data = df_cleaned_label[df_cleaned_label['Match'] == False]\n",
    "\n",
    "# Optionally, print the rows where 'Match' is False to inspect them\n",
    "print(\"Number of Wrong labels\",len(no_match_data))\n",
    "# Calculate accuracy\n",
    "accuracy = df_cleaned_label['Match'].mean()\n",
    "print(\"Accuracy\", accuracy*100)\n",
    "no_match_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50328269-9657-4412-bbc5-31810467d29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09910e0-5291-49df-b148-d2e9c9a5ff64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7339b46-41b4-4232-bed4-f79a33792a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "def process_file(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    halucination_count = data['Adjuvant Name_x'].isna().sum()\n",
    "    #print(f'Number of NaN values in Adjuvant Name_x column: {halucination_count}')\n",
    "    df_cleaned = data.dropna(subset=['Adjuvant Name_x'])\n",
    "    #df_cleaned\n",
    "\n",
    "    \n",
    "    missing_output_count = df_cleaned['Adjuvant Name_y'].isna().sum()\n",
    "\n",
    "    # Replace NaN values with \"No output Returned\"\n",
    "    df_cleaned['Adjuvant Name_y'].fillna(\"No output Returned\", inplace=True)\n",
    "\n",
    "    df_cleaned_label = df_cleaned.copy()\n",
    "\n",
    "    # Initialize the 'Match' column with False\n",
    "    df_cleaned_label['Match'] = False\n",
    "\n",
    "\n",
    "    # Function to clean and split names\n",
    "    def clean_and_split(name):\n",
    "        # Lowercase, replace hyphens with spaces, and split by \"and\"\n",
    "        return [part.strip() for part in name.split(\" and \")]\n",
    "\n",
    "    # Define a dictionary for abbreviation matching\n",
    "    abbreviation_dict = {\n",
    "        \"IFA\": \"Incomplete Freund's Adjuvant\",\n",
    "        \"Incomplete Freund's adjuvant (IFA)\": \"Incomplete Freund's Adjuvant\",\n",
    "        \"rhGM-CSF\": \"GM-CSF\",\n",
    "        \"recombinant fowlpox GM-CSF vaccine adjuvant\":\"GM-CSF\", \n",
    "        \"Sargramostim (GM-CSF)\": \"GM-CSF\",\n",
    "        \"granulocyte-macrophage colony-stimulating factor (GM-CSF)\":\"GM-CSF\", \n",
    "        \"Montanide ISA 51 VG\": \"Montanide ISA 51\",\n",
    "        \"Montanide ISA-51 VG\": \"Montanide ISA 51\",\n",
    "        \"MONTANIDE ISA 51 VG\": \"Montanide ISA 51\",\n",
    "        \"Montanide ISA51 VG\": \"Montanide ISA51\",\n",
    "        \n",
    "        \"Interleukin-2\": \"IL-2\",\n",
    "        \"polyinosinic-polycytidylic acid - poly-L-lysine carboxymethylcellulose (poly-ICLC)\": \"Poly-ICLC\",\n",
    "        \"Hiltonol (Poly-ICLC)\": \"Poly-ICLC\",\n",
    "        \"gp96 heat shock protein-peptide complex\": \"GP96\",\n",
    "        \"Therapeutic Vaccine GI-4000\":\"GI-4000\" \n",
    "        # Add more entries as needed\n",
    "    }\n",
    "    def resolve_abbreviation(name):\n",
    "        \"\"\"Resolve abbreviations using a predefined dictionary with robust matching.\"\"\"\n",
    "        # Lowercase the name for consistent dictionary lookup\n",
    "        return abbreviation_dict.get(name, name)\n",
    "\n",
    "\n",
    "    # Loop through the DataFrame and compare the values using .loc for assignment\n",
    "    for index, row in df_cleaned_label.iterrows():\n",
    "        parts_x = clean_and_split(row['Adjuvant Name_x'])\n",
    "        parts_y = clean_and_split(row['Adjuvant Name_y'])\n",
    "\n",
    "        match_found = False\n",
    "\n",
    "        for part_x in parts_x:\n",
    "            for part_y in parts_y:\n",
    "                resolved_part_x = resolve_abbreviation(part_x)\n",
    "                resolved_part_y = resolve_abbreviation(part_y)\n",
    "\n",
    "                if resolved_part_x.lower() == resolved_part_y.lower():\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif resolved_part_x.lower().replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\") == resolved_part_y.lower().replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\"):\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif resolved_part_x.lower().replace(\"-\", \" \").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\") == resolved_part_y.lower().replace(\"-\", \" \").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\"):\n",
    "                    match_found = True\n",
    "                    break\n",
    "                elif resolved_part_x.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\") == resolved_part_y.replace(\" \", \"\").lower().replace(\"-\", \"\").replace(\" adjuvant vaccine\", \"\").replace(\" adjuvant system\", \"\").replace(\" adjuvant\", \"\"):\n",
    "                    match_found = True\n",
    "                    break\n",
    "\n",
    "            if match_found:\n",
    "                break\n",
    "\n",
    "        df_cleaned_label.loc[index, 'Match'] = match_found\n",
    "        \n",
    "    return df_cleaned_label, halucination_count, missing_output_count\n",
    "\n",
    "prompt_name = [\"Prompt2_merged_0shot_postprocessed\", \"Prompt2_merged_1shot_postprocessed\", \"Prompt2_merged_2shot_postprocessed\", \"Prompt2_merged_3shot_postprocessed\", \n",
    "               \"Prompt2_merged_4shot_postprocessed\", \"Prompt2_merged_interventions_0shot_postprocessed\", \"Prompt2_merged_interventions_1shot_postprocessed\", \n",
    "               \"Prompt2_merged_interventions_2shot_postprocessed\",\"Prompt2_merged_interventions_3shot_postprocessed\", \"Prompt2_merged_interventions_4shot_postprocessed\"] \n",
    "model_name = [\"GPT_gpt-3.5-turbo-1106\", \"GPT_gpt-3.5-turbo-0125\", \"GPT_gpt-4-turbo-2024-04-09\"]\n",
    "run_no = [\"1\",\"2\",\"3\"]\n",
    "\n",
    "base_path = \"Output\"\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "no_match_data_frames = []\n",
    "\n",
    "\n",
    "# Process each file and store the results\n",
    "for prompt in prompt_name:\n",
    "    for model in model_name:\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        row_counts = []\n",
    "        halucination_counts = []\n",
    "        missing_output_counts = []\n",
    "        \n",
    "        \n",
    "        for run in run_no:\n",
    "            and_data_frames = []\n",
    "            \n",
    "            if \"interventions\" in prompt:\n",
    "                file_path = f\"{base_path}/{prompt}/{model}/AdjuvareDB104_T0.0_24June/AdjuvareDB104_T0.0_Run{run}_merged_with_goldstandard.csv\"\n",
    "            else:\n",
    "                file_path = f\"{base_path}/{prompt}/{model}/AdjuvareDB104_T0.0_18June/AdjuvareDB104_T0.0_Run{run}_merged_with_goldstandard.csv\"\n",
    "            \n",
    "            df_cleaned, halucination_count, missing_output_count = process_file(file_path)\n",
    "            df_cleaned.to_csv(f\"plot/output/df_cleaned_{prompt}_{model}_{run}.csv\", index=False)\n",
    "            \n",
    "            no_match_data = df_cleaned[df_cleaned['Match'] == False]\n",
    "            no_match_data_frames.append(no_match_data)\n",
    "            \n",
    "            and_data = df_cleaned[df_cleaned[\"Adjuvant Name_x\"].str.contains(\" and \", na=False)]\n",
    "            and_data_frames.append(and_data)\n",
    "            \n",
    "            if df_cleaned is not None:\n",
    "                accuracy = df_cleaned['Match'].mean() * 100\n",
    "                row_count = len(df_cleaned)\n",
    "                \n",
    "                \n",
    "                #### Count Missed\n",
    "                # Concatenate the list of DataFrames into a single DataFrame\n",
    "                combined_df = pd.concat(and_data_frames, ignore_index=True)\n",
    "                # Specify the column to group by\n",
    "                group_column = 'NCT Number'\n",
    "                # Grouping the combined dataframe based on the specified column\n",
    "                grouped = combined_df.groupby(group_column).size()\n",
    "                # Identifying groups where the occurrence of rows is less than two\n",
    "                missed_groups = grouped[grouped < 2]\n",
    "                #print(combined_df)\n",
    "                # Counting the total number of missed groups\n",
    "                total_missed = len(missed_groups)\n",
    "                #print(f'Total number of missed groups: {total_missed} {file_path}')\n",
    " \n",
    "                ### Count nonspecific\n",
    "                nonspecific_count_1 = (df_cleaned[\"Adjuvant Name_y\"] == \"adjuvant therapy\").sum()\n",
    "                nonspecific_count_2 = (df_cleaned[\"Adjuvant Name_y\"] == \"immunologic adjuvant\").sum()\n",
    "                nonspecific_count_3 = (df_cleaned[\"Adjuvant Name_y\"] == \"immunotherapy\").sum()\n",
    "                \n",
    "                total_nonspecific_count = nonspecific_count_1 + nonspecific_count_2+nonspecific_count_3\n",
    "                \n",
    "                \n",
    "                #Total = row_count-total_none_count+total_missed\n",
    "                Total = row_count+total_missed\n",
    "                TP = df_cleaned['Match'].sum()  # True positives\n",
    "                FP = len(df_cleaned) - TP  # False positives\n",
    "                FN = 0  \n",
    "                precision = (TP-total_nonspecific_count)/TP\n",
    "                recall = TP / Total if Total != 0 else 0\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "                results.append({\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"Model\": model,\n",
    "                    \"Run\": run,\n",
    "                    \"halucination_count\": int(halucination_count),\n",
    "                    \"missing_output_count\": int(missing_output_count),\n",
    "                    \"Accuracy\": round(accuracy, 2),\n",
    "                    \"Precision\": round(precision*100, 2),\n",
    "                    \"Recall\": round(recall*100, 2),\n",
    "                    \"F1 Score\": round(f1_score*100, 2),\n",
    "                })\n",
    "                accuracies.append(accuracy)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                f1_scores.append(f1_score)\n",
    "                row_counts.append(row_count)\n",
    "                halucination_counts.append(halucination_count)\n",
    "                missing_output_counts.append(missing_output_count)\n",
    "                \n",
    "                \n",
    "        if accuracies:\n",
    "            average_accuracy = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            average_precision = sum(precisions) / len(precisions)\n",
    "            average_recall = sum(recalls) / len(recalls)\n",
    "            average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "            \n",
    "            average_halucination_count = sum(halucination_counts) / len(halucination_counts)\n",
    "            average_missing_output_count = sum(missing_output_counts) / len(missing_output_counts)\n",
    "            \n",
    "            results.append({\n",
    "                \"Prompt\": prompt,\n",
    "                \"Model\": model,\n",
    "                \"Run\": \"Average\",\n",
    "                \"halucination_count\": round(average_halucination_count,2),\n",
    "                \"missing_output_count\": round(average_missing_output_count,2),\n",
    "                \"Accuracy\": round(average_accuracy, 2),\n",
    "                \"Precision\": round(average_precision*100, 2),\n",
    "                \"Recall\": round(average_recall*100, 2),\n",
    "                \"F1 Score\": round(average_f1_score*100, 2),\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "no_match_df = pd.concat(no_match_data_frames, ignore_index=True)\n",
    "and_df = pd.concat(and_data_frames, ignore_index=True)\n",
    "\n",
    "and_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef2311-dea5-4204-9950-dbf409d61a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb161d2-e934-42f8-a0bc-f17b73ef61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "and_df.to_csv(\"plot/and_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bb4f6-9f8e-41f6-98a6-995339d50213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results as a CSV file\n",
    "results_df.to_csv(\"plot/accuracy_and_wrong_labels_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f9e5c-13fa-44d2-ba1d-fb0a3556b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_no_match_df = no_match_df.drop_duplicates()\n",
    "unique_no_match_df.to_csv(\"plot/unique_mismatched cases.csv\", index=False)\n",
    "unique_no_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34e937-b75e-4bee-9356-796880effb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791c614-cd42-4664-8550-0a84f6de8b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b1d44-5850-4ea0-9c18-b197c729bacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443e391-9ec8-46f1-a107-a7a1a58f6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter for rows where Run is \"Average\"\n",
    "average_df = results_df[results_df[\"Run\"] == \"Average\"]\n",
    "\n",
    "# Function to plot bar chart\n",
    "def plot_bar_chart(df, value_col, y_label, title):\n",
    "    folder_name = \"plot\"\n",
    "    filename = value_col +\".png\"\n",
    "    # Construct the full path\n",
    "    full_path = os.path.join(folder_name, filename)\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    x = np.arange(len(df[\"Prompt\"].unique()))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    models = df[\"Model\"].unique()\n",
    "    offsets = (np.arange(len(models)) - len(models)/2) * width  # bar offsets for each model\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df[\"Model\"] == model]\n",
    "        plt.bar(x + offsets[i], model_data[value_col], width, label=model)\n",
    "\n",
    "    plt.xlabel(\"Prompt\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.xticks(x, df[\"Prompt\"].unique(), rotation=45, ha='right')\n",
    "    plt.legend(title=\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(full_path)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the average accuracy bar chart\n",
    "plot_bar_chart(average_df, \"Accuracy\", \"Average Accuracy (%)\", \"Average Accuracy for Each Prompt and Model Combination\")\n",
    "\n",
    "# Plot the average accuracy bar chart\n",
    "plot_bar_chart(average_df, \"Precision\", \"Average Precision (%)\", \"Average Precision for Each Prompt and Model Combination\")\n",
    "\n",
    "# Plot the average accuracy bar chart\n",
    "plot_bar_chart(average_df, \"Recall\", \"Average Recall (%)\", \"Average Recall for Each Prompt and Model Combination\")\n",
    "\n",
    "# Plot the average accuracy bar chart\n",
    "plot_bar_chart(average_df, \"F1 Score\", \"Average F1 Score (%)\", \"Average F1 Score for Each Prompt and Model Combination\")\n",
    "\n",
    "# Plot the average wrong labels count bar chart\n",
    "plot_bar_chart(average_df, \"halucination_count\", \"Halucination Count\", \"Average Halucination Count for Each Prompt and Model Combination\")\n",
    "\n",
    "\n",
    "# Plot the average wrong labels count bar chart\n",
    "plot_bar_chart(average_df, \"missing_output_count\", \"Missing Output Count\", \"Average Missing Output Count Count for Each Prompt and Model Combination\")\n",
    "\n",
    "\n",
    "# Plot the average wrong labels count bar chart\n",
    "plot_bar_chart(average_df, \"Considered Rows\", \"Considered Rows\", \"Average Considered Rows Count for Each Prompt and Model Combination\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2950e8-c293-43b8-a1b1-2578c28decbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'plot/accuracy_and_wrong_labels_results_1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter out the rows where Run is \"Average\"\n",
    "average_rows = df[df['Run'] == 'Average']\n",
    "\n",
    "# Group the data by the 'Model' column\n",
    "grouped_by_model = average_rows.groupby('Model')\n",
    "\n",
    "# Dictionary to store the highest stats for each model\n",
    "max_stats_per_model = {}\n",
    "\n",
    "# Loop through each model group\n",
    "for model, group in grouped_by_model:\n",
    "    # Find the row with the highest precision, recall, and F1 score\n",
    "    max_precision_row = group.loc[group['Precision'].idxmax()]\n",
    "    max_recall_row = group.loc[group['Recall'].idxmax()]\n",
    "    max_f1_score_row = group.loc[group['F1 Score'].idxmax()]\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    max_stats_per_model[model] = {\n",
    "        'Highest Precision': max_precision_row,\n",
    "        'Highest Recall': max_recall_row,\n",
    "        'Highest F1 Score': max_f1_score_row\n",
    "    }\n",
    "\n",
    "# Display the results for each model\n",
    "for model, stats in max_stats_per_model.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(\"\\nHighest Precision:\")\n",
    "    print(stats['Highest Precision'])\n",
    "    print(\"\\nHighest Recall:\")\n",
    "    print(stats['Highest Recall'])\n",
    "    print(\"\\nHighest F1 Score:\")\n",
    "    print(stats['Highest F1 Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff682c-2df4-49d6-8821-43351d5a6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a list to store the results for the dataframe\n",
    "results = []\n",
    "\n",
    "# Loop through each model group and append the highest stats\n",
    "for model, stats in max_stats_per_model.items():\n",
    "    # Add highest precision row\n",
    "    results.append({\n",
    "        'Model': model,\n",
    "        'Prompt': stats['Highest Precision']['Prompt'],\n",
    "        'Metric': 'Highest Precision',\n",
    "        'Precision': stats['Highest Precision']['Precision'],\n",
    "        'Recall': stats['Highest Precision']['Recall'],\n",
    "        'F1 Score': stats['Highest Precision']['F1 Score']\n",
    "    })\n",
    "    \n",
    "    # Add highest recall row\n",
    "    results.append({\n",
    "        'Model': model,\n",
    "        'Prompt': stats['Highest Recall']['Prompt'],\n",
    "        'Metric': 'Highest Recall',\n",
    "        'Precision': stats['Highest Recall']['Precision'],\n",
    "        'Recall': stats['Highest Recall']['Recall'],\n",
    "        'F1 Score': stats['Highest Recall']['F1 Score']\n",
    "    })\n",
    "    \n",
    "    # Add highest F1 score row\n",
    "    results.append({\n",
    "        'Model': model,\n",
    "        'Prompt': stats['Highest F1 Score']['Prompt'],\n",
    "        'Metric': 'Highest F1 Score',\n",
    "        'Precision': stats['Highest F1 Score']['Precision'],\n",
    "        'Recall': stats['Highest F1 Score']['Recall'],\n",
    "        'F1 Score': stats['Highest F1 Score']['F1 Score']\n",
    "    })\n",
    "\n",
    "# Convert the results into a dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282940c9-a4a8-442d-8161-31757749bb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
